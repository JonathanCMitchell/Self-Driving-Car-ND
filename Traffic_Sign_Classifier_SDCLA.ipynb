{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traffic Sign Recognition Classifier\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "from helpers import normalize_grayscale\n",
    "from helpers import transform_image\n",
    "from helpers import transform_image_data\n",
    "\n",
    "from helpers import normal_equalize\n",
    "from helpers import histogram_equalize_data\n",
    "from plotters import plot_histograms\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = 'train.p'\n",
    "testing_file = 'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**\n",
    "\n",
    "Complete the basic data summary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: 43 Test classes: 12630\n",
      "Training examples: 39209, Testing examples: 12630\n",
      "Number of training examples = 39209\n",
      "Number of training labels: (not unique):  (39209,)\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "### Replace each question mark with the appropriate value.\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = y_test.shape[0]\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "test_classes = len(np.unique(y_test))\n",
    "print('Train classes: {} Test classes: {}'.format(n_classes, n_test))\n",
    "print('Training examples: {}, Testing examples: {}'.format(n_train, n_test))\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print('Number of training labels: (not unique): ',y_train.shape)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation set: \n",
    "#### X_train: 80% , X_validation: 20% (examples)\n",
    "#### y_train: 80% , y_validation: 20% (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create validation set:, likely using sklearn\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, train_size = .80, test_size = 0.20)\n",
    "training_percentage = X_train.shape[0] / n_train\n",
    "num_training_examples = X_train.shape[0]\n",
    "num_validation_examples = X_validation.shape[0]\n",
    "assert(0.79 < training_percentage < 0.801)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read classes label data csv (signnames.csv)\n",
    "### contains: index | classId | SignName\n",
    "##### Example: 0    |   0    |    Speed limit (20km/h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_pd = pd.read_csv('signnames.csv')\n",
    "\n",
    "data_i = [[class_number, sum(y_train == class_number)] for class_number in range(len(np.unique(y_train)))]\n",
    "# Now we have a list with the indices of each class change in the y_train dataset\n",
    "# data_i is an array of tuples (class_number, # of occurances)\n",
    "\n",
    "# Now we wish to sort this list from most common occurances to least common occurances\n",
    "data_i_sorted = sorted(data_i, key = lambda i: i[1])\n",
    "data_i_sorted_array = np.array(data_i_sorted)\n",
    "\n",
    "# Use occurance as [1] and index set to the indexes as .T[0]\n",
    "data_pd['Occurance'] = pd.Series(np.asarray(data_i_sorted).T[1], index = np.asarray(data_i_sorted).T[0])\n",
    "\n",
    "# Include scaling factor\n",
    "max_abundance = np.max(np.asarray(data_i_sorted).T[1])\n",
    "scaling = np.array([max_abundance - x for x in np.asarray(data_i_sorted).T[1]])\n",
    "data_pd['Scaling_Factor'] = pd.Series(scaling.T, index = np.asarray(data_i_sorted).T[0])\n",
    "\n",
    "\n",
    "data_pd_sorted = data_pd.sort_values(['Occurance'], ascending = [0]).reset_index()\n",
    "\n",
    "# Now drop the 2nd axis titled ('index' because its annoying (stay DRY))\n",
    "data_pd_sorted = data_pd_sorted.drop('index', 1)\n",
    "\n",
    "assert(len(data_pd) == len(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_pd_sorted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create cache for unique class indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cache_unique_class_index = {}\n",
    "for i in range(len(y_train)):\n",
    "    row = data_pd_sorted.loc[data_pd_sorted['ClassId'] == y_train[i]]\n",
    "    ClassId = row['ClassId'].values[0]\n",
    "    ClassId = str(ClassId)\n",
    "    if not ClassId in cache_unique_class_index:\n",
    "        cache_unique_class_index[ClassId] = [i, row['SignName'].values[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Add in later for histogram occurence v classes graph</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (14, 10))\n",
    "# plt.bar(range(43), height = data_pd_sorted['Occurance'])\n",
    "# plt.xlabel('Classes')\n",
    "# plt.ylabel('Occurences')\n",
    "# plt.title('Occurences v Classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_class_occurence_random_sample(n_row, n_col, X, y):\n",
    "    plt.figure(figsize = (8, 8))\n",
    "    gs1 = gridspec.GridSpec(4, 4) #GridSpec(row, col)\n",
    "    gs1.update(wspace = 0.01, hspace = 0.02) #spacing bw axes\n",
    "    \n",
    "    for i in range(n_row * n_col):\n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        rand_index = np.random.randint(0, len(y)) # pick a random index in labels\n",
    "\n",
    "        plt.axis('on')\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.set_aspect('equal')\n",
    "\n",
    "        occurence = data_pd_sorted.loc[data_pd_sorted['ClassId'] == y_train[rand_index]]['Occurance'].values[0]\n",
    "        sign_name = data_pd_sorted.loc[data_pd_sorted['ClassId'] == y_train[rand_index]]['SignName'].values[0]\n",
    "        plt.text(0, 0, \"class: \" + str(y_train[rand_index]) + \"\\noccurence: \" + str(occurence) + \"\\nname: \" + str(sign_name), \n",
    "                color = 'k', backgroundcolor = 'g')\n",
    "        plt.imshow(X_train[rand_index])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Add in later for occupancy examples</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot_class_occurence_random_sample(4, 4, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, there is a big difference in the abudance of each traffic sign's class. Some signs are heavily abundant, and some are sparse. We seek to level this out by taking the sparse classes, rotating their images, and creating more of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Histogram Equalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Normal Equalization and Clahe Equalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_int:  25036\n"
     ]
    }
   ],
   "source": [
    "rand_int = np.random.randint(0, len(y_train))\n",
    "print('rand_int: ', rand_int)\n",
    "# Change image from RGB to YUV\n",
    "\n",
    "image1 = np.copy(X_train[rand_int])\n",
    "image2 = np.copy(X_train[rand_int])\n",
    "image3 = np.copy(X_train[rand_int])\n",
    "\n",
    "\n",
    "# Normal Equalization\n",
    "image2 = cv2.cvtColor(image2, cv2.COLOR_RGB2YUV)\n",
    "Yimage2 = image2[:,:,0]\n",
    "equalize_y2 = cv2.equalizeHist(Yimage2)\n",
    "image2[:,:,0] = equalize_y2\n",
    "image2RGB = cv2.cvtColor(image2, cv2.COLOR_YUV2RGB) # Back to RGB\n",
    "\n",
    "# CLAHE Equalization\n",
    "Yimage3 = image3[:,:,0]\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(6,6))\n",
    "equalize_y3 = clahe.apply(Yimage3)\n",
    "image3[:,:,0] = equalize_y3\n",
    "image3RGB = cv2.cvtColor(image2, cv2.COLOR_YUV2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Histograms to show effectiveness of histogram equalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Add in later for histogram graph </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from helpers import normal_equalize\n",
    "# from helpers import histogram_equalize_data\n",
    "# from plotters import plot_histograms\n",
    "# image2TestRGB = normal_equalize(X_train[rand_int])\n",
    "\n",
    "# plot_histograms(image1, image2TestRGB, image3RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, the regular histogram equalization and the clahe equalization seem to both work.\n",
    "In practice this reduced my error by ~1% on my validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brightness Augmentation (not performed yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek to convert the original image from RGB to HSV (Hough, Saturation, Value). We isolate the V (brightness) and increase it by a random scaling factor with uniform distribution\n",
    "\n",
    "Because at this point we have already performed max-min normalization, our values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We perform histogram equalization so we can make the brightness values more consistent within each image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Add in later for transform example</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.imshow(image2TestRGB)\n",
    "# transformed = transform_image(image2TestRGB,20, 10, 5)\n",
    "# plt.subplot(211)\n",
    "# plt.imshow(image2TestRGB)\n",
    "# plt.subplot(212)\n",
    "# plt.imshow(transformed)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform 100 images and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Add in later for transform graph</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# t_set = X_train[rand_int:rand_int + 100]\n",
    "\n",
    "# # histogram equalize first\n",
    "# t_set = histogram_equalize_data(t_set)\n",
    "# t_set = np.asarray(t_set, np.uint8)\n",
    "\n",
    "\n",
    "# gs1 = gridspec.GridSpec(10, 10)\n",
    "# gs1.update(wspace = 0.01, hspace = 0.02)\n",
    "# plt.figure(figsize = (12, 12))\n",
    "# for i in range(len(t_set)):\n",
    "#     ax1 = plt.subplot(gs1[i])\n",
    "#     ax1.set_xticklabels([])\n",
    "#     ax1.set_yticklabels([])\n",
    "#     ax1.set_aspect('equal')\n",
    "#     img = transform_image(t_set[i],20,10,5)\n",
    "\n",
    "#     plt.subplot(10,10,i+1)\n",
    "#     plt.imshow(img)\n",
    "#     plt.axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0: Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SHUFFLE LAST\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Histogram Equalization\n",
    "#### Input: (batch, 32, 32, 3)  Output: (batch, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_equalized = histogram_equalize_data(X_train)\n",
    "X_validation_equalized = histogram_equalize_data(X_validation)\n",
    "X_test_equalized = histogram_equalize_data(X_test)\n",
    "\n",
    "# Change types to uint8 for grayscaling\n",
    "X_train_equalized = X_train_equalized.astype(np.uint8)\n",
    "X_validation_equalized = X_validation_equalized.astype(np.uint8)\n",
    "X_test_equalized = X_test_equalized.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Transform, Data Creation and Brightness Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to generalize our model to an incoming test set. We ultimately want a training dataset where each class is equally represented. We do not know what the abundances of each sign class will be in our test set while we are training our model. We want our model to be able to classify any of the training images. As such we can perform the following procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: \n",
    "* Gather a histogram about the abundances of each class in the training set\n",
    "* Rotate, shear, transform, and copy each image in the training set\n",
    "\n",
    "#### Step 2:\n",
    "* Find the max abundance (max_abundance) of all the classes in the training set\n",
    "* Find the classes that are below average abundance in the training set\n",
    "* Rotate, shear, transform, and copy each of those images that are below average abundance and save to data_set_A\n",
    "* Define a scaling factor C = max_abundance - class_abundance\n",
    "* Rotate, shear, transform and copy those images C times and save to data_set_B\n",
    "* Concatenate data_set_A with data_set_B\n",
    "After this is performed we should ideally have a uniform distribution of class abundances, where each image in the set has been rotated, sheared, and transformed.\n",
    "We perform Step 1 and Step 2 on the training data, but we only perform Step 1 on the test data because we do not know the abundances of our data when we test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we perform this procedure our model will be able to generalize to any image equally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input: X_train_equalized (3137, 32, 32, 3) Output: X_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First copy and transform all the images in X_train_equalized\n",
    "# X_train_set_A = transform_image_data(X_train_equalized, ang_range = 10, shear_range = 30, trans_range = 5)\n",
    "\n",
    "# # Without histogram equalization\n",
    "# X_train_set_A = transform_image_data(X_train, ang_range = 10, shear_range = 10, trans_range = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dont transform and rotate validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from helpers import copy_and_transform_dataset\n",
    "# X_train_set_B, Y_train_set_B = copy_and_transform_dataset(x = [], y = [])\n",
    "# X_train_set_A = X_train_set_A.astype(np.uint8)\n",
    "\n",
    "# print('x_train_Set_B shape: ', X_train_set_B.shape)\n",
    "# print('y_train_set_B shape: ', Y_train_set_B.shape)\n",
    "\n",
    "\n",
    "# # # Add them together\n",
    "# X_train_added = np.append(X_train_set_A, X_train_set_B, axis = 0) # dont forget to specify axis = 0\n",
    "# y_train_added = np.append(y_train, Y_train_set_B, axis = 0)\n",
    "\n",
    "# X_train_added = X_train_added.astype(np.uint8)\n",
    "# print('X_train_added shape : ', X_train_added.dtype)\n",
    "# print('y_train_added shape :', y_train_added.dtype)\n",
    "# # y_train = y_train_added # if we want to do transformations on training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('X_train_added shape: ', X_train_added.shape)\n",
    "# print('y_train_added shape: ', y_train_added.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4,5: Grayscale + MaxMin Normalization on Train/ Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helpers import normalize_grayscale\n",
    "\n",
    "# With no transformation\n",
    "X_train_gray_data_manual = normalize_grayscale(X_train_equalized)\n",
    "X_validation_gray_data_manual = normalize_grayscale(X_validation_equalized)\n",
    "X_test_gray_data_manual = normalize_grayscale(X_test_equalized)\n",
    "\n",
    "X_train = np.expand_dims(X_train_gray_data_manual, axis = 3)\n",
    "X_validation = np.expand_dims(X_validation_gray_data_manual, axis = 3)\n",
    "X_test = np.expand_dims(X_test_gray_data_manual, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (31367, 32, 32, 1)\n",
      "y_train:  (31367,)\n",
      "X_validation:  (7842, 32, 32, 1)\n",
      "y_validation:  (7842,)\n",
      "X_test:  (12630, 32, 32, 1)\n",
      "Y_test:  (12630,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ', X_train.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('X_validation: ', X_validation.shape)\n",
    "print('y_validation: ', y_validation.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('Y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_validation.shape[0] == y_validation.shape[0])\n",
    "assert(X_test.shape[0] == y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 53\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning:\n",
    "Before I implemented histogram equalization I was able to achieve a low error withing 15 Epochs. After implementing histogram equalization I found that I was able to achieve better accuracy at ~98% with 40 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement TrafficNet-5 Layer\n",
    "### Input: (32, 32, 3) <- this may change if you grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My mindset the whole time was that I may be overfitting, and that the validation accuracy is not test accuracy, do I didn't want to be persuaded that my CNN was going to test well just because I hit 97% accuracy on a validation set, because overfitting may be the issue. Increasing epoch size to 40 -> 98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to move the weights and biases out of TrafficNet in order to normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = 0\n",
    "sigma = 0.1\n",
    "weights = {\n",
    "        'wc1': tf.Variable(tf.truncated_normal([5,5,1,6], mean = mu, stddev = sigma), name=\"weights_wc1\"),\n",
    "        'wc2': tf.Variable(tf.truncated_normal([5,5,6,16], mean = mu, stddev = sigma), name=\"weights_wc2\"),\n",
    "        'wd1': tf.Variable(tf.truncated_normal([400, 120], mean = mu, stddev = sigma), name=\"weights_wd1\"),\n",
    "        'wd2': tf.Variable(tf.truncated_normal([120, 84], mean = mu, stddev = sigma), name=\"weights_wcd2\"),\n",
    "        'out': tf.Variable(tf.truncated_normal([84, n_classes], mean = mu, stddev = sigma, name=\"weights_out\"))\n",
    "    }\n",
    "biases = {\n",
    "        'bc1': tf.Variable(tf.zeros([6]), name=\"biases_bc1\"),\n",
    "        'bc2': tf.Variable(tf.zeros([16]), name=\"biases_bc2\"),\n",
    "        'bd1': tf.Variable(tf.zeros([120]), name=\"biases_bd1\"),\n",
    "        'bd2': tf.Variable(tf.zeros([84]), name=\"biases_bd2\"),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]), name=\"biases_out\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove biases from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TrafficNet(x, weights, biases, keep_prob_l1, keep_prob_l2, keep_prob_l3, keep_prob_l4):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    # Prelayer analysis on input image\n",
    "    \n",
    "    print('PreLayer input shape: ', x.get_shape().as_list())\n",
    "    assert(x.get_shape().as_list() == [None, 32, 32, 1])\n",
    "    print('PreLayer shape verified!')\n",
    "    \n",
    "    # Layer 1: Convolutional. Input shape: [None, 32, 32, 3] Output shape:[None, 28, 28, 6]\n",
    "    \n",
    "    Layer_1 = tf.nn.conv2d(x, weights['wc1'], strides = [1,1,1,1], padding = 'VALID')\n",
    "    Layer_1 = tf.nn.bias_add(Layer_1, biases['bc1'])\n",
    "    print('Layer_1 shape: pre conv1', Layer_1.get_shape().as_list()) #14,14,6\n",
    "    assert(Layer_1.get_shape().as_list() == [None, 28, 28, 6])\n",
    "    \n",
    "    # Activation: \n",
    "    Layer_1 = tf.nn.relu(Layer_1)\n",
    "    \n",
    "    # Dropout keep_prob = 0.5\n",
    "    \n",
    "    Layer_1 = tf.nn.dropout(Layer_1,  keep_prob_l1)\n",
    "\n",
    "    # Pooling: Input shape: Input shape: [None, 28, 28, 6] Output shape: [None, 14, 14, 6]\n",
    "    k_size_p1 = [1, 2, 2, 1]\n",
    "    strides_p1 = [1, 2, 2, 1]\n",
    "    padding_p1 = 'VALID'\n",
    "    \n",
    "    Layer_1 = tf.nn.max_pool(Layer_1, k_size_p1, strides_p1, padding_p1)\n",
    "    print('Layer_1 after pool_1 shape: ', Layer_1.get_shape().as_list())\n",
    "    assert(Layer_1.get_shape().as_list() == [None, 14, 14, 6])\n",
    "    \n",
    "    # Layer 2: Convolutional: Input shape: [None, 14, 14, 6] Output shape: [None, 10, 10, 16]\n",
    "    Layer_2 = tf.nn.conv2d(Layer_1, weights['wc2'], strides = [1,1,1,1], padding = 'VALID') + biases['bc2']\n",
    "    Layer_2 = tf.nn.bias_add(Layer_2, biases['bc2'])\n",
    "    print('Layer_2 before pool_2 shape: ', Layer_2.get_shape().as_list())\n",
    "    assert(Layer_2.get_shape().as_list() == [None, 10, 10, 16])    \n",
    "    \n",
    "    # Activation:\n",
    "    \n",
    "    Layer_2 = tf.nn.relu(Layer_2)\n",
    "    \n",
    "    # Dropout:\n",
    "    \n",
    "    Layer_2 = tf.nn.dropout(Layer_2,  keep_prob_l2)\n",
    "\n",
    "    # Pooling: Input shape: [None, 10, 10, 16] Output shape: [None, 5, 5, 16]\n",
    "    \n",
    "    Layer_2 = tf.nn.max_pool(Layer_2, [1,2,2,1], [1,2,2,1], 'VALID')\n",
    "    print('Layer_2 after pool_2 shape: ', Layer_2.get_shape().as_list())\n",
    "    assert(Layer_2.get_shape().as_list() == [None, 5, 5, 16])\n",
    "    \n",
    "    # Flatten: Input shape: [None, 5, 5, 16] Output shape: [None, 1, 5*5*16]\n",
    "    \n",
    "    Flatten_1 = tf.reshape(Layer_2, [-1, 400])\n",
    "    print('Flatten_1 shape: ', Flatten_1.get_shape().as_list())\n",
    "    assert(Flatten_1.get_shape().as_list() == [None, 400])\n",
    "    \n",
    "    # Layer 3: Fully Connected: Input shape: [None, 1, 5*5*16] Output shape: [None, 120]\n",
    "    \n",
    "    Layer_3_FC1 = tf.add(tf.matmul(Flatten_1, weights['wd1']), biases['bd1'])\n",
    "    print('Layer_3_FC1 : ',  Layer_3_FC1.get_shape().as_list())\n",
    "    assert(Layer_3_FC1.get_shape().as_list() == [None, 120])\n",
    "    \n",
    "    # Activation: \n",
    "    \n",
    "    Layer_3_FC1 = tf.nn.relu(Layer_3_FC1)\n",
    "    \n",
    "    # Dropout: keep_prob = 0.8\n",
    "    \n",
    "    Layer_3_FC1 = tf.nn.dropout(Layer_3_FC1,  keep_prob_l3)\n",
    "    \n",
    "    # TODO Layer 4: Fully Connected: Input shape: [None, 120] Output shape: [None, 84]\n",
    "    \n",
    "    Layer_4_FC2 = tf.add(tf.matmul(Layer_3_FC1, weights['wd2']), biases['bd2'])\n",
    "    print('Layer_4_FC2: ', Layer_4_FC2.get_shape().as_list())\n",
    "    assert(Layer_4_FC2.get_shape().as_list() == [None, 84])\n",
    "    \n",
    "    # Activation\n",
    "    \n",
    "    Layer_4_FC2 = tf.nn.relu(Layer_4_FC2)\n",
    "    \n",
    "    # TODO Dropout: keep_prob = 0.7\n",
    "    # Fact: Had better validation set accuracy before dropout! was at 97.5 , but dropout prevents overfitting\n",
    "    Layer_4_FC2 = tf.nn.dropout(Layer_4_FC2, keep_prob_l4)\n",
    "    \n",
    "    # Layer 5: Fully Connected (Logits): Input shape: [None, 84] Output_shape: [None, 43]\n",
    "    \n",
    "    Layer_5_FC3 = tf.add(tf.matmul(Layer_4_FC2, weights['out']), biases['out'])\n",
    "    print('Layer_5_FC3: ', Layer_5_FC3.get_shape().as_list())\n",
    "    assert(Layer_5_FC3.get_shape().as_list() == [None, n_classes])\n",
    "    logits = Layer_5_FC3\n",
    "    \n",
    "    # Output: Logits shape: [None, 43]\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "Train TrafficNet to classify Traffic-Sign data\n",
    "x is a placeholder for a batch of input images, y is a placeholder for a batch of output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Create the graph variables\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 1]) #unpack tuple in case we convert to grayscale\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "keep_prob_l1 = tf.placeholder(tf.float32)\n",
    "keep_prob_l2 = tf.placeholder(tf.float32)\n",
    "keep_prob_l3 = tf.placeholder(tf.float32)\n",
    "keep_prob_l4 = tf.placeholder(tf.float32)\n",
    "\n",
    "test_type = tf.placeholder(tf.string)\n",
    "one_hot_y = tf.one_hot(y, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "Create a training pipeline that uses the model to classify Traffic Sign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreLayer input shape:  [None, 32, 32, 1]\n",
      "PreLayer shape verified!\n",
      "Layer_1 shape: pre conv1 [None, 28, 28, 6]\n",
      "Layer_1 after pool_1 shape:  [None, 14, 14, 6]\n",
      "Layer_2 before pool_2 shape:  [None, 10, 10, 16]\n",
      "Layer_2 after pool_2 shape:  [None, 5, 5, 16]\n",
      "Flatten_1 shape:  [None, 400]\n",
      "Layer_3_FC1 :  [None, 120]\n",
      "Layer_4_FC2:  [None, 84]\n",
      "Layer_5_FC3:  [None, 43]\n",
      "logits shape:  Tensor(\"Add_14:0\", shape=(?, 43), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Here we create the model parameters\n",
    "rate = 0.001\n",
    "β = 0.001\n",
    "\n",
    "logits = TrafficNet(x, weights, biases, keep_prob_l1, keep_prob_l2, keep_prob_l3, keep_prob_l4)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy) \\\n",
    "+ β*tf.nn.l2_loss(weights['wd1']) \\\n",
    "+ β*tf.nn.l2_loss(weights['wd2']) \\\n",
    "# + β*tf.nn.l2_loss(weights['out'])\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Evaluate loss and accuracy of the model for a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #note: casting just changes the type\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "labels_pred = logits\n",
    "labels_pred_cls = tf.argmax(logits, 1)\n",
    "def evaluate(X_data, y_data):\n",
    "    \n",
    "    \n",
    "    \n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset: offset + BATCH_SIZE], y_data[offset: offset + BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict = { x: batch_x, y: batch_y, \n",
    "                    keep_prob_l1: 1,\n",
    "                    keep_prob_l2: 1,\n",
    "                    keep_prob_l3: 1,\n",
    "                    keep_prob_l4: 1\n",
    "        })\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Run the training data throug the training data pipeline to train the model\n",
    "<ul>\n",
    "<li>Before each epoch, shuffle the training set</li>\n",
    "<li>After each epoch, measure the loss and accuracy of the validation set </li>\n",
    "<li>Save the model after training</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data for plotting\n",
    "EPOCHS_STORE = []\n",
    "Accuracies = []\n",
    "save_file = 'traffic-test-lala'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.486\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.740\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.798\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.844\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.873\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.892\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.893\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.918\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.926\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.932\n",
      "\n",
      "EPOCH 11 ...\n",
      "Validation Accuracy = 0.941\n",
      "\n",
      "EPOCH 12 ...\n",
      "Validation Accuracy = 0.946\n",
      "\n",
      "EPOCH 13 ...\n",
      "Validation Accuracy = 0.949\n",
      "\n",
      "EPOCH 14 ...\n",
      "Validation Accuracy = 0.945\n",
      "\n",
      "EPOCH 15 ...\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "EPOCH 16 ...\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "EPOCH 17 ...\n",
      "Validation Accuracy = 0.955\n",
      "\n",
      "EPOCH 18 ...\n",
      "Validation Accuracy = 0.960\n",
      "\n",
      "EPOCH 19 ...\n",
      "Validation Accuracy = 0.961\n",
      "\n",
      "EPOCH 20 ...\n",
      "Validation Accuracy = 0.960\n",
      "\n",
      "EPOCH 21 ...\n",
      "Validation Accuracy = 0.965\n",
      "\n",
      "EPOCH 22 ...\n",
      "Validation Accuracy = 0.970\n",
      "\n",
      "EPOCH 23 ...\n",
      "Validation Accuracy = 0.969\n",
      "\n",
      "EPOCH 24 ...\n",
      "Validation Accuracy = 0.969\n",
      "\n",
      "EPOCH 25 ...\n",
      "Validation Accuracy = 0.971\n",
      "\n",
      "EPOCH 26 ...\n",
      "Validation Accuracy = 0.973\n",
      "\n",
      "EPOCH 27 ...\n",
      "Validation Accuracy = 0.974\n",
      "\n",
      "EPOCH 28 ...\n",
      "Validation Accuracy = 0.971\n",
      "\n",
      "EPOCH 29 ...\n",
      "Validation Accuracy = 0.975\n",
      "\n",
      "EPOCH 30 ...\n",
      "Validation Accuracy = 0.975\n",
      "\n",
      "EPOCH 31 ...\n",
      "Validation Accuracy = 0.976\n",
      "\n",
      "EPOCH 32 ...\n",
      "Validation Accuracy = 0.977\n",
      "\n",
      "EPOCH 33 ...\n",
      "Validation Accuracy = 0.977\n",
      "\n",
      "EPOCH 34 ...\n",
      "Validation Accuracy = 0.977\n",
      "\n",
      "EPOCH 35 ...\n",
      "Validation Accuracy = 0.975\n",
      "\n",
      "EPOCH 36 ...\n",
      "Validation Accuracy = 0.978\n",
      "\n",
      "EPOCH 37 ...\n",
      "Validation Accuracy = 0.976\n",
      "\n",
      "EPOCH 38 ...\n",
      "Validation Accuracy = 0.979\n",
      "\n",
      "EPOCH 39 ...\n",
      "Validation Accuracy = 0.975\n",
      "\n",
      "EPOCH 40 ...\n",
      "Validation Accuracy = 0.978\n",
      "\n",
      "EPOCH 41 ...\n",
      "Validation Accuracy = 0.976\n",
      "\n",
      "EPOCH 42 ...\n",
      "Validation Accuracy = 0.980\n",
      "\n",
      "EPOCH 43 ...\n",
      "Validation Accuracy = 0.972\n",
      "\n",
      "EPOCH 44 ...\n",
      "Validation Accuracy = 0.982\n",
      "\n",
      "EPOCH 45 ...\n",
      "Validation Accuracy = 0.982\n",
      "\n",
      "EPOCH 46 ...\n",
      "Validation Accuracy = 0.980\n",
      "\n",
      "EPOCH 47 ...\n",
      "Validation Accuracy = 0.977\n",
      "\n",
      "EPOCH 48 ...\n",
      "Validation Accuracy = 0.981\n",
      "\n",
      "EPOCH 49 ...\n",
      "Validation Accuracy = 0.981\n",
      "\n",
      "EPOCH 50 ...\n",
      "Validation Accuracy = 0.981\n",
      "\n",
      "EPOCH 51 ...\n",
      "Validation Accuracy = 0.980\n",
      "\n",
      "EPOCH 52 ...\n",
      "Validation Accuracy = 0.980\n",
      "\n",
      "EPOCH 53 ...\n",
      "Validation Accuracy = 0.984\n",
      "\n",
      "Model saved in file: %s traffic-test-lala\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print('Training...')\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        # Shuffle the training values before training\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x_train, batch_y_train = X_train[offset:end], y_train[offset: end]\n",
    "            sess.run(training_operation, feed_dict = {x: batch_x_train, y: batch_y_train, \n",
    "                     keep_prob_l1: 0.9,\n",
    "                     keep_prob_l2: 0.8,\n",
    "                     keep_prob_l3: 0.7,\n",
    "                     keep_prob_l4: 0.5\n",
    "               \n",
    "            })        \n",
    "        validation_accuracy = evaluate(X_validation, y_validation) # create with sklearn \n",
    "        EPOCHS_STORE.append(i+1)\n",
    "        Accuracies.append(validation_accuracy)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "    saver.save(sess, save_file)\n",
    "    print('Model saved in file: %s', save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAGHCAYAAAD/QltcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzMzK40LDzMzMyuNCw8zMzMrzX8BHUgrltYKv84AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbda015b278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print('EPOCHS: ', EPOCHS)\n",
    "# print('ACCURACIES: ', len(Accuracies))\n",
    "\n",
    "plt.title('Epochs v Error')\n",
    "Error_scores = [1 - a for a in Accuracies]\n",
    "plt.plot(EPOCHS_STORE, Error_scores)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "max_idx = np.argmax(Accuracies)\n",
    "plt.annotate('max accuracy: '+ str(Accuracies[max_idx] * 100) + '%', xy=(max_idx, Error_scores[max_idx]), xytext=(16, .18),\n",
    "            arrowprops=dict(facecolor='black', shrink = 0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on Test Set\n",
    "As you can see, we really start to get marginal differences after epoch 12. The error rate also oscillates, increasing and decreasing with each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ls new_signs/images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "new_images = []\n",
    "for name in glob.glob('new_signs/images/*'):\n",
    "#     print('name is : ', name)\n",
    "    img = mpimg.imread(name)\n",
    "#     print('img shape: ', img.shape)\n",
    "    new_images.append(img)\n",
    "    \n",
    "\n",
    "print('type: ', type(new_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "(281, 281, 4)\n"
     ]
    }
   ],
   "source": [
    "nra = np.array(new_images)\n",
    "print(nra.shape)\n",
    "\n",
    "print(nra[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgg = mpimg.imread('new_signs/images/EZPass.png')\n",
    "# remember matplotlib imports as RGB and OpenCV imports as BGR\n",
    "# plt.imshow(imgg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgcv = cv2.imread('new_signs/images/EZPass.png')\n",
    "imgcv = cv2.cvtColor(imgcv, cv2.COLOR_BGR2RGB)\n",
    "# plt.imshow(imgcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.930\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax_2:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(tf.argmax(logits, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test shape:  (12630, 32, 32, 1)\n",
      "X_test_sample shape:  (9, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X test shape: ', X_test.shape)\n",
    "X_test_sample = X_test[1:10]\n",
    "print('X_test_sample shape: ', X_test_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopKV2(values=array([[  9.96103048e-01,   1.62324950e-03,   1.02253677e-03],\n",
      "       [  1.00000000e+00,   3.62589542e-11,   2.93348733e-13],\n",
      "       [  9.98300135e-01,   1.69304665e-03,   6.92103549e-06],\n",
      "       [  1.00000000e+00,   8.98261587e-12,   2.21496389e-12],\n",
      "       [  1.00000000e+00,   9.06766162e-10,   2.26888549e-14],\n",
      "       [  9.99260604e-01,   7.39433046e-04,   5.66942671e-09],\n",
      "       [  9.97733712e-01,   2.18068203e-03,   6.43196254e-05],\n",
      "       [  9.99997377e-01,   1.13280373e-06,   7.72822602e-07],\n",
      "       [  9.97844696e-01,   2.05377536e-03,   9.61739570e-05]], dtype=float32), indices=array([[ 1, 14,  5],\n",
      "       [38, 34, 20],\n",
      "       [33, 39, 25],\n",
      "       [11,  1,  6],\n",
      "       [38, 15,  5],\n",
      "       [18, 26, 40],\n",
      "       [12,  7, 42],\n",
      "       [25, 24, 27],\n",
      "       [35,  3,  9]], dtype=int32))\n",
      "Test Accuracy = 0.930\n"
     ]
    }
   ],
   "source": [
    "softmax_logits = tf.nn.softmax(logits)\n",
    "top_k = tf.nn.top_k(softmax_logits, k=3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    my_softmax_logits = sess.run(softmax_logits, feed_dict = {x : X_test_sample, keep_prob_l1: 0.9,\n",
    "                     keep_prob_l2: 0.8,\n",
    "                     keep_prob_l3: 0.7,\n",
    "                     keep_prob_l4: 0.5 })\n",
    "    my_k = sess.run(top_k, feed_dict = {x : X_test_sample, keep_prob_l1: 0.9,\n",
    "                     keep_prob_l2: 0.8,\n",
    "                     keep_prob_l3: 0.7,\n",
    "                     keep_prob_l4: 0.5 })\n",
    "    print(my_k)\n",
    "#     fig, axs = plt.subplots(len(X_test_sample),4, figsize=(12, 14))\n",
    "#     fig.subplots_adjust(hspace = .4, wspace=.2)\n",
    "#     axs = axs.ravel()\n",
    "\n",
    "#     for i, image in enumerate(X_test_sample):\n",
    "#         axs[4*i].axis('off')\n",
    "#         axs[4*i].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "#         axs[4*i].set_title('input')\n",
    "#         guess1 = my_top_k[1][i][0]\n",
    "#         index1 = np.argwhere(y_validation == guess1)[0]\n",
    "#         axs[4*i+1].axis('off')\n",
    "#         axs[4*i+1].imshow(X_validation[index1].squeeze(), cmap='gray')\n",
    "#         axs[4*i+1].set_title('top guess: {} ({:.0f}%)'.format(guess1, 100*my_top_k[0][i][0]))\n",
    "#         guess2 = my_top_k[1][i][1]\n",
    "#         index2 = np.argwhere(y_validation == guess2)[0]\n",
    "#         axs[4*i+2].axis('off')\n",
    "#         axs[4*i+2].imshow(X_validation[index2].squeeze(), cmap='gray')\n",
    "#         axs[4*i+2].set_title('2nd guess: {} ({:.0f}%)'.format(guess2, 100*my_top_k[0][i][1]))\n",
    "#         guess3 = my_top_k[1][i][2]\n",
    "#         index3 = np.argwhere(y_validation == guess3)[0]\n",
    "#         axs[4*i+3].axis('off')\n",
    "#         axs[4*i+3].imshow(X_validation[index3].squeeze(), cmap='gray')\n",
    "#         axs[4*i+3].set_title('3rd guess: {} ({:.0f}%)'.format(guess3, 100*my_top_k[0][i][2]))\n",
    "    \n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLANATIONS \n",
    "# Question 1:\n",
    "_Describe how you preprocessed the data. Why did you choose that technique?_\n",
    "\n",
    "### Answer:\n",
    "My first conclusion was that I needed to perform histogram equalization before I grayscale + normalize because grayscale + normalization results in data loss. Even though it is slower to perform equalization over three layers, I didn't want to lose information about the data set.\n",
    "\n",
    "#### Histogram Equalization + Brightness augmentation order:\n",
    "I added brightness augmentation after histogram equalization. After we equalize the pixel distribution it appears \"more normal\". Then we add the randomness factor of brightness augmentation by increasing random brightness values to random parts of the image. This introduces a randomness factor which will prevent our model from overfitting because the model is being trained on images with random brightness values. If we performed brightness augmentation first followed by histogram equalization we would end up with a roughly \"normalized\" contrast distribution, but it would be normalized not from the original data, but from the original data + brightness augmentation. This would cause our normalization curve to shift to a false location.\n",
    "\n",
    "## Steps: \n",
    "### 1) Histogram Equalization:\n",
    "I applied a contrast limited adaptive histogram equalization to prevent overamplification of noise that adaptive histogram equalization can give rise to, (loss of information due to over brightness). This noise-amplification may occur if the histogram is not confined to a particular region in the image. \n",
    "\n",
    "CLAHE limits the amplification by clipping the histogram at a predifined value before computing the Cumulative Distribution Function, limiting the slope of the CDF -> limits the transform function.\n",
    "\n",
    "I utilized OpenCV's clahe class structure with a clipLimit = , and a tileGridSize = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Affine transformation and data creation\n",
    "When we transform image data we won't know which dataset will be lacking in our test or validation set so we simply create additional data for every data point in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Rotate, Transform, Jitter: Create Additional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see in the histogram we have uneven traffic sign abundances. We seek to generalize our data for any sign in the data set, and to not specifically train our model any specific traffic sign. To do this we wil Rotate, Jitter, Transform and create additional traffic sign models. These affine transformations will be performed in OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One way to create more data is to change the camera angle of the sign and apply affine warping with OpenCV and submit that as another data point.\n",
    "We apply a random uniform distribution constant to our warping parameters in order to maintain randomness and attempt to give equal probability to the degree of each transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we transform image data we won't know which dataset will be lacking in our test or validation set so we simply create additional data for every data point in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tested, this method proved to give me 2% less accuracy then when I didn't transform. This could be due to flattening out the histogram so there was a uniform distribution of each sign's abundance. In reality, traffic signs are not uniformally distributed among the streets, so we shouldn't have a uniform set when training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brightness Augmentation (not performed yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Brightness Augmentation:\n",
    "I chose to convert RGB to YUV. Y = Luma (Brightness). This allows me to only adjust the luminance without distorting the color information. I chose to do this before grayscaling because grayscaling causes me to lose information by averaging the channels and converging them, (and I hope the RGB2YUV conversion doesn't). \n",
    "\n",
    "Grayscaling averages all the color channels and converges them to one channel while YUV maintains the same depth. Using just Y (luma) is different than using the grayscaled values because grayscaled values are affected by the colors, (flattened) whereas YUV allows us the maintain the colors and only augment the luminance (Y). For this same reason I perform Histogram Equalization only on the Y value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek to convert the original image from RGB to HSV (Hough, Saturation, Value). We isolate the V (brightness) and increase it by a random scaling factor with uniform distribution\n",
    "\n",
    "Because at this point we have already performed max-min normalization, our values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek to convert the original image from RGB to HSV (Hough, Saturation, Value). We isolate the V (brightness) and increase it by a random scaling factor with uniform distribution\n",
    "\n",
    "Because at this point we have already performed max-min normalization, our values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We perform histogram equalization so we can make the brightness values more consistent within each image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Grayscaling:\n",
    "First I grayscaled my data so that I could perform min-max scaling. I read online that Yan Le-Cun said having color did not increase performance that much so I decided to grayscale all the images.\n",
    "Next came the normalization technique:\n",
    "### 5) Normalization:\n",
    "I chose to implement min-max scaling because it scaled my images from [0, 255] to a range [0, 1]. This is great for images specifically because other methods like mean variance normalization can create negative values which will then get dropped after my activation function. I didn't want to lose data and I still wanted normalized values.\n",
    "<ul>Initially I used sklearn's preprocessing library, specifically the MinMax_Scaler function to normalize my data. I then implemented my own min-max scaling function and started testing which one gave me a better accuracy on the validation set. I found that with sklearns library I got 96.5% accuracy. With my manual function I tweaked the parameters as follows:\n",
    "<li>a = 0.1, b = 0.9. accuracy = 92.4%</li>\n",
    "<li>a = 0.05, b = 0.95 accuracy = 96.8 (at this point I chose to keep this method)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "As it turns out I performed much much better on my validation training set when I did not augment or create additional data. When augmenting I was getting around ~92% accuracy on my validation set, when I just used the data given and did not transform, rotate, or shear it I received ~99% accuracy on my validation set. I don't know what my test accuracy would have been if I used the augmented data but I assume it would not have been as high.\n",
    "\n",
    "This may be due to the augmentation itself. When transforming and shearing the images I replace what was image pixels with black pixels, those black pixels can disrupt my dataset because I do not expect to see such black pixels in the test and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Describe how you set up the training, validation and testing data for your model. Optional: If you generated additional data, how did you generate the data? Why did you generate the data? What are the differences in the new dataset (with generated data) from the original dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Run the training data throug the training data pipeline to train the model\n",
    "<ul>\n",
    "<li>Before each epoch, shuffle the training set</li>\n",
    "<li>After each epoch, measure the loss and accuracy of the validation set </li>\n",
    "<li>Save the model after training</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)  For reference on how to build a deep neural network using TensorFlow, see [Deep Neural Network in TensorFlow\n",
    "](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/b516a270-8600-4f93-a0a3-20dfeabe5da6/concepts/83a3a2a2-a9bd-4b7b-95b0-eb924ab14432) from the classroom._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Layer 1: Convolutional. Input shape: [None, 32, 32, 3] Output shape:[None, 28, 28, 6]\n",
    "  \n",
    " #### Activation: \n",
    "    \n",
    " #### Pooling: Input shape: Input shape: [None, 28, 28, 6] Output shape: [None, 14, 14, 6]\n",
    "    \n",
    " #### Layer 2: Convolutional: Input shape: [None, 14, 14, 6] Output shape: [None, 10, 10, 16]\n",
    "    \n",
    " #### Activation:\n",
    "    \n",
    " #### Pooling: Input shape: [None, 10, 10, 16] Output shape: [None, 5, 5, 16]\n",
    "    \n",
    " #### Flatten: Input shape: [None, 5, 5, 16] Output shape: [None, 1, 5*5*16]\n",
    "    \n",
    " #### Layer 3: Fully Connected: Input shape: [None, 1, 5*5*16] Output shape: [None, 120]\n",
    "    \n",
    " #### Activation: \n",
    " \n",
    " #### TODO : Dropout\n",
    "    \n",
    " #### Layer 4: Fully Connected: Input shape: [None, 120] Output shape: [None, 84]\n",
    "    \n",
    " #### Activation\n",
    "    \n",
    " #### Layer 5: Fully Connected (Logits): Input shape: [None, 84] Output_shape: [None, 43]\n",
    "    \n",
    " #### Output: Logits shape: [None, 43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "### Dropout:\n",
    "At first, I only implemented dropout on the fully connected layers and ignored the convolutional layers because they have few parameters. After some testing I increased my validation accuracy by around 1.5% with this method. At first I thought that because we are averaging the gradients in our convolutional layers that performing dropout will not prevent overfitting because we have already interconnected our feature maps by averaging the gradients over their spatial extent. So I just thought it would slow down training.\n",
    "\n",
    "Then after reading [1] I noticed that they claimed an additional performance of 3.05% to 2.55% by adding dropout in the convolutional layers. My original assumption of preventing dropout in convolutional layers due to their parametric sparsity proved to be incorrect. Dropout in convolutional layers helps because it provides noisy inputs for the higher fully connected layes - preventing them from overfitting. I used keep_prob = 0.7 for the convolutional layers and 0.5 for the fully connected layers.\n",
    "\n",
    "<ul>First stage: \n",
    "<li>Dropout in Layer_3_FullyConnected_1: Keep_prob = 0.8</li>\n",
    "<li>Dropout in Layer_4_FullyConnected_2: keep_prob = 0.667</li>\n",
    "<li>Validation Accuracy = 97% (may be overfitting)</li>\n",
    "</ul>\n",
    "<ul>Second stage:\n",
    "<li>Dropout in Layer_1_Conv: keep_prob = 0.5</li>\n",
    "<li>Dropout in Layer_3_FullyConnected_1: Keep_prob = 0.8</li>\n",
    "<li>Dropout in Layer_4_FullyConnected_2: keep_prob = 0.667</li>\n",
    "<li>Validation Accuracy = 96.4 (Dropped 0.6%)</li>\n",
    "<li>Although my validation accuracy dropped 0.6% by implementing dropout in my first layer, it didn't drop enough to give me a poor validation accuracy and it will likely prevent overfitting.</li>\n",
    "</ul>\n",
    "\n",
    "<ul>Third stage:\n",
    "<li>Dropout in Layer_1_Conv: keep_prob = 0.9</li>\n",
    "<li>Dropout in Layer_2_Conv: keep_prob = 0.8</li>\n",
    "<li>Dropout in Layer_3_FullyConnected_1: Keep_prob = 0.7</li>\n",
    "<li>Dropout in Layer_4_FullyConnected_2: keep_prob = 0.5</li>\n",
    "<li>Validation Accuracy = 96.4 (Dropped 0.6%)</li>\n",
    "<li>Although my validation accuracy dropped 0.6% by implementing dropout in my first layer, it didn't drop enough to give me a poor validation accuracy and it will likely prevent overfitting.</li>\n",
    "</ul>\n",
    "\n",
    "<ul>Fourth stage:\n",
    "<li>After I took out all the other dropout's and just left Dropout in Layer_4 at 0.5 I get validation accuracy of 94.3% in 10 epochs but 98.3% in 50 epochs</li>\n",
    "<li>Dropout in Layer_1_Conv: keep_prob = 0.9</li>\n",
    "<li>Dropout in Layer_2_Conv: keep_prob = 0.8</li>\n",
    "<li>Dropout in Layer_3_FullyConnected_1: Keep_prob = 0.7</li>\n",
    "<li>Dropout in Layer_4_FullyConnected_2: keep_prob = 0.5</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My mindset the whole time was that I may be overfitting, and that the validation accuracy is not test accuracy, do I didn't want to be persuaded that my CNN was going to test well just because I hit 97% accuracy on a validation set, because overfitting may be the issue. Increasing epoch size to 40 -> 98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "_How did you train your model? (Type of optimizer, batch size, epochs, hyperparameters, etc.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the graph above, when we use 145 Epochs and a batch size of 256 we achieved ~99% accuracy on validation data. I increased my batch size from 128 to 256 because I have a larger GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply L2 Regularization\n",
    "Apply L2Reg only on the weights, not the biases.\n",
    "<ul>\n",
    "<li>At first I implemented L2 Regularization on all the weights giving me a validation set accuracy of 97%</li> \n",
    "<li>Then I implemented it only on the fully connected layers, leading to a validation set accuracy of 97.1%, but this could just be to better starting weight initialization values</li>\n",
    "<li>After testing L2 Regularization only on the fully connected layers I received a validation set accuracy of 97.5% so I kept this method</li>\n",
    "<li>After testing L2 Regularization on the convolutional layers as well it seems that there is not much improvement because normalizing the fully connected layers retroactively normalizes the convolutional layers as well</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('EPOCHS: ', EPOCHS)\n",
    "# print('ACCURACIES: ', len(Accuracies))\n",
    "\n",
    "plt.title('Epochs v Error')\n",
    "Error_scores = [1 - a for a in Accuracies]\n",
    "plt.plot(EPOCHS_STORE, Error_scores)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "max_idx = np.argmax(Accuracies)\n",
    "plt.annotate('max accuracy: '+ str(Accuracies[max_idx] * 100) + '%', xy=(max_idx, Error_scores[max_idx]), xytext=(16, .18),\n",
    "            arrowprops=dict(facecolor='black', shrink = 0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis:\n",
    "As you can see, we really start to get marginal differences after epoch 12. The error rate also oscillates, increasing and decreasing with each epoch.\n",
    "At first I was using ~40 Epochs. Then I was using ~140 epochs and I noticed that I was oscillating between 98.7% and 99%. So I decided to trim it down to ~80 epochs where I knew I would be about 98%. This was I prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Test Accuracy: 93.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "_What approach did you take in coming up with a solution to this problem? It may have been a process of trial and error, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think this is suitable for the current problem._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "Take several pictures of traffic signs that you find on the web or around you (at least five), and run them through your classifier on your computer to produce example results. The classifier might not recognize some local signs but it could prove interesting nonetheless.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "_Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult? It could be helpful to plot the images in the notebook._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run the predictions here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "_Is your model able to perform equally well on captured pictures when compared to testing on the dataset? The simplest way to do this check the accuracy of the predictions. For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate._\n",
    "\n",
    "_**NOTE:** You could check the accuracy manually by using `signnames.csv` (same directory). This file has a mapping from the class id (0-42) to the corresponding sign name. So, you could take the class id the model outputs, lookup the name in `signnames.csv` and see if it matches the sign from the image._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize the softmax probabilities here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "*Use the model's softmax probabilities to visualize the **certainty** of its predictions, [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#top_k) could prove helpful here. Which predictions is the model certain of? Uncertain? If the model was incorrect in its initial prediction, does the correct prediction appear in the top k? (k should be 5 at most)*\n",
    "\n",
    "`tf.nn.top_k` will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it'll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids.\n",
    "\n",
    "Take this numpy array as an example:\n",
    "\n",
    "```\n",
    "# (5, 6) array\n",
    "a = np.array([[ 0.24879643,  0.07032244,  0.12641572,  0.34763842,  0.07893497,\n",
    "         0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.08594638,  0.0178669 ,  0.18063401,\n",
    "         0.15899337],\n",
    "       [ 0.26076848,  0.23664738,  0.08020603,  0.07001922,  0.1134371 ,\n",
    "         0.23892179],\n",
    "       [ 0.11943333,  0.29198961,  0.02605103,  0.26234032,  0.1351348 ,\n",
    "         0.16505091],\n",
    "       [ 0.09561176,  0.34396535,  0.0643941 ,  0.16240774,  0.24206137,\n",
    "         0.09155967]])\n",
    "```\n",
    "\n",
    "Running it through `sess.run(tf.nn.top_k(tf.constant(a), k=3))` produces:\n",
    "\n",
    "```\n",
    "TopKV2(values=array([[ 0.34763842,  0.24879643,  0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.18063401],\n",
    "       [ 0.26076848,  0.23892179,  0.23664738],\n",
    "       [ 0.29198961,  0.26234032,  0.16505091],\n",
    "       [ 0.34396535,  0.24206137,  0.16240774]]), indices=array([[3, 0, 5],\n",
    "       [0, 1, 4],\n",
    "       [0, 5, 1],\n",
    "       [1, 3, 5],\n",
    "       [1, 4, 3]], dtype=int32))\n",
    "```\n",
    "\n",
    "Looking just at the first row we get `[ 0.34763842,  0.24879643,  0.12789202]`, you can confirm these are the 3 largest probabilities in `a`. You'll also notice `[3, 0, 5]` are the corresponding indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
